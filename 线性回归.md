# 线性回归
## 1.回归算法
### 1.1 概念
回归算法主要用于机器学习中的监督学习，当输入带有标签的自变量，程序会返回连续的目标值。该算法常基于大量数据集对未知进行预测，是机器学习中的典型算法。
### 1.2 回归模型
- 线性回归
- 多项式回归
- 回归树
- 支持向量机（SVM）
- K均值回归
- 神经网络回归
- 集成回归  
## 2.线性回归
### 2.1 概念
线性回归是一种寻找自变量与因变量线性关系的算法，即自变量的次数为1。它通过一条直线来拟合数据，寻求误差的最小化.     
![alt text](<屏幕截图 2025-12-16 225600.png>)
### 2.2 简单线性回归
自变量单一的线性回归模型
形如
$$
f_{w,b}(x)=wx+b 
$$
### 2.3 成本函数
用于判断回归模型拟合的效果好坏，成本函数越小，说明模型对原数据的拟合程度较高。
公式： 
$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^m \left( f_{w, b}(x^{(i)}) - y^{(i)} \right)^2
$$
除2m是方便求导时可以消去2，使式子更简洁。  
可以利用成本函数来寻找适合的w,b,使模型的拟合效果更好。
![alt text](<屏幕截图 2025-12-17 161453.png>)  

可以通过可视化函数直观呈现  
![alt text](<屏幕截图 2025-12-17 162034.png>)
### 2.4 梯度下降算法
梯度下降常用于寻找模型的参数，如简单线性回归的w,b。  
原理：修改参数并计算成本函数，并与未修改前的进行比较，寻求使成本函数全局最小或局部最小的参数。  
![alt text](<屏幕截图 2025-12-17 163212.png>)
如同从山顶下山，要每一步都选择向下的方向，才有可能到达最低处。  
![alt text](<屏幕截图 2025-12-17 164304.png>)
注意w,b要同时变化，不能用新的w去求b,二者的变化应该是同步的。下图是成本函数偏导数的推导
![alt text](image.png)  
学习率决定变化的大小，而偏导数决定了变化的方向。学习率去正数，太小使计算时间过长，太大可能会使一次变化过大，导致成本函数来回在最小值左右变化，无法收敛。
### 2.5 梯度下降算法的代码实现
````python
import numpy as np
import matplotlib.pyplot as plt
data = np.array([[34, 58], [42, 65], [55, 72], [49, 81], [61, 89], [38, 63], [52, 77], [46, 71], [57, 84], [43, 68],
[50, 73], [59, 92], [47, 69], [53, 79], [41, 62], [56, 87], [44, 67], [62, 95], [39, 61], [51, 76],
[48, 74], [54, 83], [58, 91], [45, 70], [63, 98], [40, 64], [49, 78], [55, 85], [42, 66], [60, 94]])     #数据集
x=data[:,0]
y=data[:,1]      #分别读取两列的数据
def comput_cost(w,b,data):
    cost=0
    l=len(data)
    for i in range(l):
        x=data[i,0]
        y=data[i,1]
        cost+=(w*x+b-y)**2
    average_cost=cost/l       #计算成本
    return average_cost
alpha=0.0001
initial_w=0
initial_b=0
num_iter=15             #初始化w,b,学习率，计算次数
def grad_desc(data,initial_w,initial_b,alpha,num_iter):
    w=initial_w
    b=initial_b
    cost_list=[]
    for i in range(num_iter):
        cost_list.append(comput_cost(w,b,data))
        w,b=step_grad_desc(w,b,alpha,data)
    return [w,b,cost_list]
def step_grad_desc(current_w,current_b,alpha,data):
    sum_grad_w=0
    sum_grad_b=0
    l=len(data)
    for i in range(l):
        x=data[i,0]
        y=data[i,1]
        sum_grad_w+=(current_w*x+current_b-y)*x
        sum_grad_b +=current_w * x + current_b - y       #偏导数
    grad_w=2/l*sum_grad_w
    grad_b=2/l*sum_grad_b
    update_w=current_w-alpha*grad_w
    update_b=current_b-alpha*grad_b              #分别计算新的w,b
    return update_w,update_b                       #返回w,b
w,b,cost_list=grad_desc(data,initial_w, initial_b, alpha, num_iter)
cost=comput_cost(w,b,data)
print(cost_list)
plt.plot(cost_list)
plt.show()
plt.scatter(x,y)              #绘制散点图
pred_y=w*x+b
plt.plot(x,pred_y,c='r')  #绘制图像
plt.show()



````
运行结果  
![alt text](image-1.png)
![alt text](image-2.png)
### 2.6 多元线性回归
特征值为多个，即相比于简单线性回归有多个自变量。多元线性回归对数据集的拟合效果更好。
公式：  
$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \cdots + \beta_p x_p + \epsilon
$$
### 2.7 线性回归的向量化
向量化比较简洁，高效。其对两个向量进行点积操作，由于是并行处理，所以速度较快。
对比图  
![alt text](image-3.png)
### 2.8 正规方程（最小二乘法）
无需经过迭代，可以直接求出回归方程中的参数。但无法运用到其他机器学习算法中，如神经网络，决策树。   
推导思路：   
1.对参数偏导  
2.令导函数为零  
3.联立求解  
代码实现 （简单线性回归） 
```python
import numpy as np
import matplotlib.pyplot as plt
data = np.array([[32,31],[53,68],[61,62],[47,71],[59,87],[55,78],[52,79],[39,59],[48,75],[52,71],
          [45,55],[54,82],[44,62],[58,75],[56,81],[48,60],[44,82],[60,97],[45, 48],[38,56],
          [66,83],[65,118],[47,57],[41,51],[51,75],[59,74],[57,95],[63,95],[46,79],[50,83]])
x=data[:,0]
y=data[:,1]
#plt.scatter(x,y)
#plt.show()
def compute_cost(w,b,points):
    total_cost=0;
    l=len(points)
    for i in range(l):
        x=points[i,0]
        y=points[i,1]
        total_cost+=(x*w+b-y)**2
    return total_cost/l
def average(data):
    sum=0
    num=len(data)
    for i in range(num):
          sum+=data[i]
    return sum/num
def fit(points):
    M=len(points)
    x_bar=average(points[:,0])
    sum_yx=0
    sum_x2=0
    sum_delta=0
    sum_x=0
    sum_y=0

    for i in range(M):
        x=points[i,0]
        y=points[i,1]
        sum_yx+=y*(x-x_bar)
        sum_x2+=x**2
    w=sum_yx/(sum_x2-M*(x_bar**2))
    for i in range(M):
        x=points[i,0]
        y=points[i,1]
        sum_delta+=y-w*x
    b=sum_delta/M
    return w,b
w,b=fit(data)
plt.scatter(x,y)
pred_y=w*x+b
plt.plot(x,pred_y,c="r")
plt.show()
```
### 2.9 多元梯度下降
多元梯度下降的逻辑与简单线性回归的梯度下降差不多，本质都是不断更新w,b，使成本函数不断减小。  
### 2.10 特征缩放
如果不同特征值数值差异较大，可能让成本函数图像呈现椭圆形，需要对参数计算多次，导致梯度下降速度较慢。
![alt text](image-4.png)
此时就需要特征缩放把特征值限定到一个范围，可以采用归一化操作处理数据。  
数据处理常见操作：  
1.除以最大值  
2.均值缩放  
3.Z-score缩放  

除以最大值
![alt text](image-5.png)

均值缩放
![alt text](image-6.png)

Z-score缩放
![alt text](image-7.png)
### 2.11 学习率的选择
学习率如果太大，有可能使参数在最有值左右来回震荡。学习率太小，会使迭代次数过多。一般可以先设置不同大小的学习率，根据代价函数变化的图像来确定相对最优的学习率
![alt text](image-8.png)
### 2.12 特征工程
通过变化和组合原有特征来创造新特征，以更好创造模型。
![alt text](image-9.png)
### 2.13 多项式回归
线性回归的拓展形式，本质上是线性的，对参数线性。注意要对数据进行特征缩放，因为不同次幂的特征值数值差异可能很大。回归模型较复杂，要注意过拟合的问题。